# ğŸš€ Data Engineering Project â€“ Databricks (Bronze â†’ Silver â†’ Gold Pipeline)

This project showcases an end-to-end **data engineering pipeline** built on **Databricks**, using **AWS S3**, **Unity Catalog**, **Delta Lake**, and **Lakeflow Jobs**.  
It demonstrates how a **Child Company** processes data and shares curated **Gold** datasets with a **Parent Company** for enterprise-level analytics.

---

## ğŸ—ï¸ Architecture Overview

The workflow follows the Medallion Architecture pattern:

### 1ï¸âƒ£ RAW Data (S3)
- ğŸ“¥ Raw data lands in an S3 bucket (JSON/CSV).  
- ğŸ—„ï¸ Historical data archived for auditing.

### 2ï¸âƒ£ Bronze Layer
- âš™ï¸ Data ingestion using **Lakeflow Jobs**.  
- ğŸ§¹ Basic parsing and minimal transformation.

### 3ï¸âƒ£ Silver Layer
- ğŸ”§ Data cleaning, validation, and schema standardization.  
- ğŸ”„ Business rules applied.

### 4ï¸âƒ£ Gold Layer
- â­ Child Company produces analytics-ready Gold tables.  
- ğŸ”— Shared with Parent Company via **Unity Catalog**.  
- ğŸ¢ Parent Company merges child Gold into enterprise analytics layers.

### 5ï¸âƒ£ Serving Layer
- ğŸ“Š Dashboards  
- ğŸ§ Genie  
- ğŸ¤– BI & Analytics Applications  

---

## ğŸ§° Tech Stack
- ğŸ”¥ Databricks  
- ğŸ± Unity Catalog  
- ğŸ’¾ Delta Lake  
- ğŸ”„ Lakeflow Jobs  
- â˜ï¸ AWS S3  
- ğŸ Python / PySpark  
- ğŸ›¢ï¸ SQL  

